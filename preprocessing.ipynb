{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\n\n\nimport re\nimport string\nimport inflect\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nimport nltk\nimport csv\nimport gc\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv(\"../input/amazon-ml-challenge-2021-hackerearth/train.csv\",escapechar = \"\\\\\",error_bad_lines=False,quoting = csv.QUOTE_NONE)\ntest=pd.read_csv(\"../input/amazon-ml-challenge-2021-hackerearth/test.csv\",escapechar = \"\\\\\",error_bad_lines=False,quoting = csv.QUOTE_NONE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# combining only relevant column for features\ntrain['Final']=train['TITLE']+\" \"+train[\"BRAND\"]\ntest['Final']=test[\"TITLE\"]+\" \"+test[\"BRAND\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#drop unnecessary columns\ntrain.drop(['TITLE','DESCRIPTION','BULLET_POINTS','BRAND'],axis=1,inplace=True)\ntest.drop(['TITLE','DESCRIPTION','BULLET_POINTS','BRAND'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's handle null values\ntrain.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.fillna('unknown',inplace=True)\ntest.fillna('unknown',inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#clean text data\ndef remove_whitespace(text):\n    return \" \".join(text.split())\n\ndef remove_url(text):\n    return re.sub(r'http|https|www|xxx\\S+', '', text)\n\ndef remove_single_char(text):\n    return re.sub('(\\\\b[A-Za-z] \\\\b|\\\\b [A-Za-z]\\\\b)', '', text)\n\ndef valid_character_filtering(text):\n    return \"\".join(filter(lambda char: char in string.printable, text))\n  \ndef remove_special_char(text):\n    return re.sub(r\"[^a-zA-Z0-9]\",\" \",text)\n\ndef number_to_text(text):\n    p = inflect.engine()\n\n    temp_str = text.split()\n    new_string = []\n\n    for word in temp_str:\n        if word.isdigit():\n            temp = p.number_to_words(word)\n            new_string.append(temp)\n\n        else:\n            new_string.append(word)\n\n    temp_str = ' '.join(new_string)\n    return temp_str\n\ndef remove_long_number(text):\n    result = re.sub(r'\\d+', '', text)\n    return result\n\ndef remove_punctuation(text):\n    result=re.sub(r\"[^\\w\\s]\",\" \",text)\n    return result\n\ndef remove_stopwords(text):\n    stop_words = set(stopwords.words(\"english\"))\n    word_tokens = word_tokenize(text)\n    filtered_text = [word for word in word_tokens if word not in stop_words]\n    return filtered_text\n\ndef lemmatize_word(text):\n    lemmatizer = WordNetLemmatizer()\n    lemmas = [lemmatizer.lemmatize(word, pos='v') for word in text]\n    return lemmas\n\ndef sentence(text):\n    return ' '.join(word for word in text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Final']=train['Final'].apply(lambda x:x.lower())\ntrain['Final']=train['Final'].apply(remove_whitespace)\ntrain['Final']=train['Final'].apply(remove_url)\ntrain['Final']=train['Final'].apply(remove_special_char)\ntrain['Final']=train['Final'].apply(valid_character_filtering)\ntrain['Final']=train['Final'].apply(number_to_text)\ntrain['Final']=train['Final'].apply(remove_long_number)\ntrain['Final']=train['Final'].apply(remove_punctuation)\ntrain['Final']=train['Final'].apply(remove_stopwords)\ntrain['Final']=train['Final'].apply(lemmatize_word)\ntrain['Final']=train['Final'].apply(sentence)\ntrain['Final']=train['Final'].apply(remove_single_char)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['Final']=test['Final'].apply(lambda x:x.lower())\ntest['Final']=test['Final'].apply(remove_whitespace)\ntest['Final']=test['Final'].apply(remove_url)\ntest['Final']=test['Final'].apply(remove_special_char)\ntest['Final']=test['Final'].apply(valid_character_filtering)\ntest['Final']=test['Final'].apply(number_to_text)\ntest['Final']=test['Final'].apply(remove_long_number)\ntest['Final']=test['Final'].apply(remove_punctuation)\ntest['Final']=test['Final'].apply(remove_stopwords)\ntest['Final']=test['Final'].apply(lemmatize_word)\ntest['Final']=test['Final'].apply(sentence)\ntest['Final']=test['Final'].apply(remove_single_char)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the preocessed data in csv formate\ntrain.to_csv(\"./train_processed.csv\",index=False)\ntest.to_csv(\"./test_processed.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}